 Tiejun Zhao is supported by the National Natural Science Foundation of China (NSFC) via grant 91520204 and National High Technology Research & Development Program of China (863 program) via grant 2015AA015405.  In this paper, we explored the source dependency information to improve the performance of NMT.  These verify that the proposed double-context method is effective for word prediction.  In this paper, we propose a novel NMT with source dependency representation to improve translation performance.  , hJ) are used to generate the target word in the Decoder.  Our neural network consists of an input layer, two convolutional layers, two pooling layers and an output layer: • Input layer: the input layer takes words of a dependency unitUj in the form of embedding vectors n×d, where n is the number of words in a dependency unit and d is vector dimension of each word.  In our experiments, we set n to 10,1 and d is 620.  4.2 SDRNMT-2 In SDRNMT-1, a single annotation, learned over concatenating word representation and SDR, is used to compute the context vector and the RNN hidden state for the current time step.  Source annotation vectors are learned based on the concatenated representation with dependency information: hj = fenc(Vxj : VUj , hj−1), (8) where “:” denotes the operation of vectors concatenation.  (13) Finally, according to eq.  We shuffle training set before training and the mini-batch size is 80.  In this section, we propose two novel NMT models SDRNMT-1 and SDRNMT-2, both of which can make use of source dependency information SDR to enhance Encoder and Decoder of NMT.  This work is partially supported by the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of MIC, Japan.  The training dataset consists of 1.42M 2λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments.  The AttNMT significantly outperforms PBSMT by 2.74 BLEU points on average, indicating that it is a strong baseline NMT system.  The word embedding dimension is 620,5 and the hidden layer dimension the 260 dimensions are from VUj .  The baseline Sennrichdeponly improves the performance over the AttNMT by 0.58 BLEU points on average.  We follow (Bahdanau et al., 2014) to group sentences of similar lengths all the test sets (MT03-08), for example, “40” indicates that the length of sentences is between 30 and 40, and compute a BLEU score per group.  (12) The current hidden state ssi and s d i are computed by eq.  For all NMT systems, we limit the source and target vocabularies to 30K, and the maximum sentence length is 80.  In the future, we will try to exploit a general framework for utilizing richer syntax knowledge.  Especially, the proposed SDRNMT2 outperforms the AttNMT and Sennrich-deponly on average by 1.64 and 1.03 BLEU points.  sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese.  (6), given the previous hidden state ssi−1 and s d i−1, the current alignments esi,j and e d i,j are computed over source annotation vectors hj and dj , respectively: esi,j = f(s s i−1 + hj), edi,j = f(s d i−1 + dj).  Table 1 shows the translation performances on test sets measured in BLEU score.  To relieve more translation performance for NMT from the SDR, we propose a double-context mechanism, as shown in Figure 3.  We design a simplified neural network following Chen et al. (2017)’s Convolutional Neural Network (CNN) method, to learn the SDR for each source dependency unit Uj , as shown in Figure 1. 
