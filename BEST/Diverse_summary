 Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b).  Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT.  In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction.  In this paper, we propose a novel NMT with source dependency representation to improve translation performance.  Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information.  Then we design an Encoder with convolutional architecture to jointly learn SDRs (Section 3) and source dependency annotations, thus computing dependency context vectors and hidden states by a novel double-context based Decoder for word prediction (Section 4).  Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves significant gains over the method by Sennrich and Haddow (2016), and thus delivers substantial improvements over the standard attentional NMT (Section 5).  An NMT model consists of an Encoder process and a Decoder process, and hence it is often called Encoder-Decoder model (Sutskever et al., 2014; Bahdanau et al., 2014).  , xJ) is firstly embedded as a vector Vxj , and then represented as 2846 an annotation vector hj by hj = fenc(Vxj , hj−1), (1) where fenc is a bidirectional Recurrent Neural Network (RNN) (Bahdanau et al., 2014).  These annotation vectors H = (h1, .  An RNN Decoder is used to compute the target word yi probability by a softmax layer g: p(yi|y<i, x) = g(ŷi−1, si, ci), (2) where ŷi−1 is the previously emitted word, and si is an RNN hidden state for the current time step: si = ϕ(ŷi−1, si−1, ci), (3) and the context vector ci is computed as a weighted sum of these source annotations hj : ci = J∑ j=1 αijhj , (4) where the normalized alignment weight αij is computed by αij = exp(eij)∑J k=1 exp(eik) , (5) where eij is an alignment which indicates how well the inputs around position j and the output at the position i match: eij = f(si−1, hj).  In order to capture source long-distance dependency constraints, we extract a dependency unit Uj for each source word xj from dependency tree, inspired by a dependency-based bilingual composition sequence for SMT (Chen et al., 2017).  Our neural network consists of an input layer, two convolutional layers, two pooling layers and an output layer: • Input layer: the input layer takes words of a dependency unitUj in the form of embedding vectors n×d, where n is the number of words in a dependency unit and d is vector dimension of each word.  • Max-Pooling layer: the first pooling layer performs row-wise max over the two consecutive rows to output a n−24 ×d matrix; the second pooling layer performs row-wise max over the two consecutive rows to output a n−2 8 ×d matrix.  To relieve more translation performance for NMT from the SDR, we propose a double-context mechanism, as shown in Figure 3.  The training dataset consists of 1.42M 2λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments.  Training is conducted on a single Tesla P100 GPU.  These results show that the proposed models can effective encode longdistance dependencies to improve translation.  We proposed a novel attentional NMT with source dependency representation to capture source longdistance dependencies.  In the future, we will try to exploit a general framework for utilizing richer syntax knowledge. 
